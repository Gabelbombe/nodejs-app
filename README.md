# Build, Test and Deploy with Terraform, Packer and Jenkins

For this project I wanted to create a completely automated build and deploy system that is built on scalable infrastructure with immutable servers.



#### What is NOT Immutable?

In traditional server methodology is where we would use something like a _Configuration Management_ tool to take an existing server, let you to specify how said servers should be configured and bring new and existing machines into compliance. This helps to avoid the problem of fragile _Snowflakes_.

Automated configuration tools (Chef, Puppet etc) are usually used with _Configuration Synchronization_ where you leave a server running for a potentially long period of time, repeatedly applying configuration to bring it into line with the latest specification. *In theory* servers can be allowed to run indefinitely and they'll be kept completely consistent and up to date. In practice it's not possible to manage a servers configuration completely, so there is considerable scope for configuration drift, and unexpected changes to running servers. We call this server drift, among other things.



#### So... What is Immutable?

An immutable server is built to not accept any changes once it is launched and provisioned. This means you should not be modifying your credentials, revising your code or deploy new files to rollout an update for your application. To rollout an update, new server groups with updated code will be created, old ones will be terminated and then deployed in place of the old ones. This way, it becomes easier you auto scale your application with one true source (our AWS AMI) to launch new instances to meet your workload.

As an added bonus, by frequently destroying and rebuilding servers from the base image, 100% of the servers elements are reset to a known state, without spending a ridiculous amount of time specifying and maintaining detailed configuration specifications.



#### Requirements

To run this example/setup we'll need a (working) Jenkins server, a GitHub account and AWS account. I also assume that you have working VPC with associated subnets that can already communicate with the outside world. I designed this so that most of the resources will fit into AWS free tier, so you should (theoretically) not be charged if you destroy everything after completion.



#### Cool story, so what now?

We'll be using a modern DevOps toolset to build this out, I'll go through the step by step process that I took in order to create the complete setup. For now let's start with a simple NodeJS (Express web app) to set up our deployment.



##### Your Web Code (ExpressJS)

We will use default template generated by express generator for our NodejS web application.

 1. Install NodeJS on your system (please note version which you are installing, as we will use the same version in further configuration. For now we're using 8.9.4 LTS)
 2. Generate template using `express nodejs-app`. This will create a project skeleton. Check [express generator docs](https://expressjs.com/en/starter/generator.html) for help.
 3. Push your code to a GitHub repository. You can use other central repository if you wish so. (Note: make sure you've ignored `node_modules` through your `.gitignore` file.



#### Our Docker Container

We will create a custom image with Packer, Terraform and latest Git installed already setup. Please find Dockerfile below.
```docker
FROM node:8.9.4 AS build-container
ENV DISTR='linux_amd64'
ENV P_VER='1.1.3'
ENV T_VER='0.11.1'
ENV G_VER='2.9.5'


RUN [ "/bin/bash", "-c", "mkdir -p /root/{packer,git}" ]

WORKDIR /root/packer
RUN  wget https://releases.hashicorp.com/packer/${P_VER}/packer_${P_VER}_${DISTR}.zip       \
  && wget https://releases.hashicorp.com/terraform/${T_VER}/terraform_${T_VER}_${DISTR}.zip \
  && wget https://github.com/git/git/archive/v${G_VER}.zip

RUN  apt-get update                                                             \
  && apt-get install -y unzip build-essential libssl-dev libcurl4-gnutls-dev    \
                        libexpat1-dev gettext                                   \
  && rm -rf /var/lib/apt/lists/*

RUN  unzip packer_${P_VER}_${DISTR}.zip                                         \
  && unzip terraform_${T_VER}_${DISTR}.zip                                      \
  && unzip v${G_VER}.zip                                                        \
  && rm *.zip

RUN  mv packer        /usr/local/bin/packer                                     \
  && mv terraform     /usr/local/bin/terraform                                  \
  && mv git-${G_VER}  /root/git


WORKDIR /root/git/git-${G_VER}
RUN  make configure                                                             \
  && ./configure --prefix=/usr                                                  \
  && make all                                                                   \
  && make install


## Oikology
RUN [ "/bin/bash", "-c", "rm -fr /root/{packer,git}" ]
```

We can save ourselves here from creating the above custom image since I already published built and published it for public use on [DockerHub](https://hub.docker.com/r/ehime/build-container/). I will also use my public image from DockerHub in this setup.



#### Generate AWS Key and Secret Key

Generate [AWS Key and Secret Key](https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html) for your AWS account for your root account or an IAM user with permission to manage resources like EC2, Elastic Load Balancer (ELB), Auto Scaling Group (ASG), Security Group (SG) and Launch Configuration (LC). You will need this key and secret to manage resource using Terraform and Packer.



#### Packer Configurations

I am using [Packer](https://www.packer.io/docs/index.html) to create native AWS AMI which will be used in the Launch Configuration for the Auto Scaling Group which will be attached to the Load Balancer.

We will need to configure our machines by installing applications like NodeJS, copy our project code and configuring auto-startup of server. Then using [Ansbile Local provisioner](https://www.packer.io/docs/provisioners/ansible-local.html) to configure everything so our application will start when the machine boots up.

Packer is used to bake everything into the native AWS AMI. Below is the complete packer configuration

```hcl
{
  "variables": {
    "aws_access_key": "",
    "aws_secret_key": "",
    "is_local": "false"
  },

  "provisioners": [{
      "type": "shell",
      "execute_command": "echo 'ubuntu' | {{ .Vars }} sudo -E -S sh '{{ .Path }}'",
      "inline": [
          "sleep 30",
          "apt-add-repository -y ppa:ansible/ansible",
          "/usr/bin/apt-get update",
          "/usr/bin/apt-get -y install ansible",
          "mkdir /home/ubuntu/nodejs-app",
          "chown -R ubuntu:ubuntu /home/ubuntu/nodejs-app"
      ]
    }, {
      "type": "file",
      "source": ".",
      "destination": "/home/ubuntu/nodejs-app/"
    }, {
      "type": "ansible-local",
      "playbook_file": "packer/packer.yml",
      "extra_arguments": [ "--extra-vars \"is_local={{user `is_local`}}\"" ]
    }, {
      "type": "shell",
      "inline": [
        "mkdir -p /tmp/tests"
      ]
    }, {
        "type": "file",
        "source": "packer/tests",
        "destination": "/tmp/tests"
    }, {
        "type": "shell",
        "script": "packer/scripts/serverspec.sh"
    }
  ],

  "builders": [{
    "type": "amazon-ebs",
    "access_key": "{{user `aws_access_key`}}",
    "secret_key": "{{user `aws_secret_key`}}",
    "region": "us-west-2",
    "source_ami_filter": {
      "filters": {
        "virtualization-type": "hvm",
        "name": "ubuntu/images/*ubuntu-trusty-14.04-amd64-server-*",
        "root-device-type": "ebs"
      },
      "owners": ["099720109477"],
      "most_recent": true
    },
    "instance_type": "t2.micro",
    "ssh_username": "ubuntu",
    "ami_name": "packer-example-{{user `ami_prefix`}}-{{user `ami_type`}}-1604-{{isotime \"2006-01-02-1504\"}}-{{user `git_hash`}}",
    "launch_block_device_mappings": [{
      "device_name": "/dev/sda1",
      "volume_size": 8,
      "volume_type": "gp2",
      "delete_on_termination": true
    }]
  }],

  "post-processors": [{
      "type": "vagrant",
      "compression_level": 9,
      "output": "../builds/{{.Provider}}/packer_amazon-ebs.box"
  }]
}
```

We are using five provisioners here, made up of four parts

 - Shell provisioner to install Ansible and create project directory `/home/ubuntu/nodejs-app`.
 - File provisioner to copy project code.
 - Ansible provisioner to configure our machine.
 - Test syncing and kickstart for ServerSpec testing of the AMI


Our Ansible playbook file looks like below:

```yaml
---
- hosts: all

  pre_tasks:

    # Force latest Python2 for incompat fix
    - name: Update legacy Python2
      raw: sudo apt-get -y install python-simplejson

     # Force BirghtBox Ruby installation
    - name: Add latest repo for Ruby
      become: true
      apt_repository:
        repo: ppa:brightbox/ruby-ng


  tasks:

    # Ruby stuff
    - name: Latest version of Ruby is installed
      become: true
      apt:
        name: "{{ item }}"
      with_items:
        - ruby2.3
        - ruby2.3-dev

    - name: Install Gems with proper $PATH
      become: true
      gem:
        name: "{{ item }}"
        user_install: no
      with_items:
        - safe_yaml   # ServerSpec Dep
        - serverspec
        - specinfra
        - rake


    # Install the NodeJS pieces
    - name: Install the gpg key for nodejs LTS
      become: true
      apt_key:
        url: "http://deb.nodesource.com/gpgkey/nodesource.gpg.key"
        state: present
        validate_certs: no

    - name: Install the nodejs LTS repos
      become: true
      apt_repository:
        repo: "deb http://deb.nodesource.com/node_8.x {{ ansible_distribution_release }} main"
        state: present
        update_cache: yes

    - name: Install NodeJs
      become: true
      apt:
        name: nodejs
        state: present

    - name: Install PM2
      become: true
      command: npm install -g pm2

    - name: Create PM2 service
      become: true
      command: env PATH=$PATH:/usr/bin pm2 startup upstart -u ubuntu --hp /home/ubuntu


    # If this is a local build lets do it a bit diff for testing...
    - name: "Install GIT"
      become: true
      apt:
        name: git
      state: present
      when:
        - is_local == "true"

    - name: "Clone our source repository"
      become: true
      become_user: ubuntu
      command: git clone https://github.com/ehime/nodejs-app.git /home/ubuntu/nodejs-app
      when:
        - is_local == "true"

    - name: "Move testing scripts"
      become: true
      become_user: ubuntu
      command: mv /home/ubuntu/nodejs-app/packer/tests /tmp/tests
      when:
        - is_local == "true"
    # End our case...


    - name: PM2 startup configuration (pm2 start)
      command: /usr/bin/pm2 start bin/www --name="nodejs-app"
      args:
        chdir: /home/ubuntu/nodejs-app/

    - name: PM2 startup configuration (pm2 save)
      command: /usr/bin/pm2 save
```

We've used [AWS amazon-ebs builder](https://www.packer.io/docs/builders/amazon-ebs.html) to create a builder. You should be able to understand configuration by reading out, but we are essentially pushing this to EBS post build.

After creating a new folder in our project folder with name `packer` (this will help segment this application into easily understandable pieces) and store these files with name `packer.json` and `packer.yml`.



#### Terraform Configurations

```hcl
variable "access_key" {}
variable "secret_key" {}
variable "region" {}

provider "aws" {
  access_key = "${var.access_key}"
  secret_key = "${var.secret_key}"
  region     = "${var.region}"
}

data "aws_ami" "nodejs_app_ami" {
  most_recent = true

  filter {
    name   = "name"
    values = ["packer-example*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  owners = ["015583679202"]
}

resource "aws_launch_configuration" "nodejs_app_lc" {
  image_id        = "${data.aws_ami.nodejs_app_ami.id}"
  instance_type   = "t2.micro"
  security_groups = ["${aws_security_group.nodejs_app_websg.id}"]

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_autoscaling_group" "nodejs_app_asg" {
  name                 = "terraform-asg-nodejs-app-${aws_launch_configuration.nodejs_app_lc.name}"
  launch_configuration = "${aws_launch_configuration.nodejs_app_lc.name}"
  availability_zones   = ["${data.aws_availability_zones.allzones.names}"]
  min_size             = 1
  max_size             = 2

  load_balancers    = ["${aws_elb.elb1.id}"]
  health_check_type = "ELB"

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_security_group" "nodejs_app_websg" {
  name = "security_group_for_nodejs_app_websg"

  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_security_group" "elbsg" {
  name = "security_group_for_elb"

  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  lifecycle {
    create_before_destroy = true
  }
}

data "aws_availability_zones" "allzones" {}

resource "aws_elb" "elb1" {
  name               = "terraform-elb-nodejs-app"
  availability_zones = ["${data.aws_availability_zones.allzones.names}"]
  security_groups    = ["${aws_security_group.elbsg.id}"]

  listener {
    instance_port     = 3000
    instance_protocol = "http"
    lb_port           = 3000
    lb_protocol       = "http"
  }

  health_check {
    healthy_threshold   = 2
    unhealthy_threshold = 2
    timeout             = 3
    target              = "HTTP:3000/"
    interval            = 30
  }

  cross_zone_load_balancing   = true
  idle_timeout                = 400
  connection_draining         = true
  connection_draining_timeout = 400

  tags {
    Name = "terraform - elb - nodejs-app"
  }
}
```

When you apply these configurations, Terraform will use latest AMI (notice `most_recent = true`) and will modify AWS resources to use new AMI. Executing `terraform apply` will modify and recreate some of the resources in a way so create happens before destroy (notice `create_before_destroy`). You can also extract AMI id from Packer

Some important points to note here are:

 - Both LC and ASG have `create_before_destroy` set to `true`
 - The LC omits the name attribute to allow Terraform to auto-generate a random one, which prevent collisions
 - The ASG interpolates the launch configuration name into its name, so LC changes always force replacement of the ASG (and not just an ASG update).
 - The ASG sets `min_elb_capicity` which means Terraform will wait for instances in the new ASG to show up as `InService` in the ELB before considering the ASG successfully created.

The behavior when AMI changes is:

 - New LC is created with the fresh AMI
 - New ASG is created with the fresh LC
 - Terraform waits for the new ASG's instances to spin up and attach to the ELB
 - Once all new instances are InService, Terraform begins destroy of old ASG
 - Once old ASG is destroyed, Terraform destroys old LC

Note: Terraform uses a file `${region}_nodejs-app.tfstate` in our `_state` directory to store all the mappings and states of AWS resources it manages. We need to get this file before we run terraform apply again. Using Jenkins, we will be updating this file to our repository (a separate GitHub repository) to preserve the states created by Terraform.


#### Jenkins Configuration

Aah ha! finally we go for automating everything using Jenkins. Create a new pipeline for your GitHub project in Jenkins which you created for NodeJS express.


Our `Jenkinsfile` looks like below:

```jenkins
pipeline {
  agent {
    docker {
      image 'ehime/build-container:latest'
    }
  }
  stages {
      stage('Build') {
        steps {
          sh 'npm install'
        }
      }

      stage('Create Packer AMI') {
          steps {
            withCredentials([
              usernamePassword(credentialsId: 'ada90a34-30ef-47fb-8a7f-a97fe69ff93f', passwordVariable: 'AWS_SECRET', usernameVariable: 'AWS_KEY')
            ]) {
              sh 'packer build -var aws_access_key=${AWS_KEY} -var aws_secret_key=${AWS_SECRET} packer/packer.json'
          }
        }
      }
      stage('Application Unit Tests') {
        steps {
          sh 'echo -e "You should insert your steps for REAL testing here-ish..'
        }
      }
      stage('Amazon Services Deployment') {
        steps {
            withCredentials([
              usernamePassword(credentialsId: 'ada90a34-30ef-47fb-8a7f-a97fe69ff93f', passwordVariable: 'AWS_SECRET', usernameVariable: 'AWS_KEY'),
              usernamePassword(credentialsId: '2facaea2-613b-4f34-9fb7-1dc2daf25c45', passwordVariable: 'REPO_PASS',  usernameVariable: 'REPO_USER'),
            ]) {
              sh '[ -d nodejs-app-terraform ] && rm -rf nodejs-app-terraform'
              sh 'git clone https://github.com/ehime/nodejs-app-terraform.git'
              sh '''
                 cd nodejs-app-terraform
                 terraform init
                 terraform apply -auto-approve -var access_key=${AWS_KEY} -var secret_key=${AWS_SECRET} -var region=${REGION}
                 git add _state/*
                 git -c user.name="ehime" -c user.email="dodomeki@gmail.com" commit -m "TF-State update from Jenkins"
                 git push https://${REPO_USER}:${REPO_PASS}@github.com/ehime/nodejs-app-terraform.git master
              '''
          }
      }
    }

    environment {
      npm_config_cache = 'npm-cache'
    }
  }
}
```

We've divided whole process into 4 simple stages

 - Build
 - Create Packer AMI (`packer` part)
 - Test or discard Image (`packer` part)
 - AWS Deployment (`terraform` part)


We are using our custom docker image ehime/build-container from Docker Hub.

Also note that, we are pushing back our terraform state file each time after successful terraform apply run. It is important because Terraform stores mappings it has created in this file and need it while applying changes.



#### Result

If you've connected all the wires correctly. You should be able to access your web server using your ELB DNS on 3000 port.


#### Complete Code

    NodeJs (Web sever code) which includes Packer and Ansbile configuration under packer folder.
    https://github.com/ehime/nodejs-app
    Terraform Repository
    https://github.com/ehime/nodejs-app-terraform
